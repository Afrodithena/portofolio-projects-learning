{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c2d1b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c766fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=14khOIntWRDhbrE82xsz1iLTLn6P_DoQx\n",
      "From (redirected): https://drive.google.com/uc?id=14khOIntWRDhbrE82xsz1iLTLn6P_DoQx&confirm=t&uuid=93986da9-d810-4fbf-bd1e-5fdc05160adf\n",
      "To: d:\\portfolio-projects-learning\\data.csv\n",
      "100%|██████████| 419M/419M [00:22<00:00, 19.0MB/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m output = \u001b[33m'\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m gdown.download(url, output, quiet=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m data.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "url = \"https://drive.google.com/uc?id=14khOIntWRDhbrE82xsz1iLTLn6P_DoQx\"\n",
    "output = 'data.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(data):\n",
    "  missing_count = data.isnull().sum()\n",
    "  missing_percent = missing_count * 100 / len(data)\n",
    "  missing_table = pd.concat([missing_count, missing_percent], axis=1)\n",
    "  missing_table.columns = ['Missing Count', 'Missing Percentage']\n",
    "  missing_table.sort_values(by='Missing Percentage', ascending=False, inplace = True)\n",
    "  return missing_table\n",
    "missing_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370fd6e",
   "metadata": {},
   "source": [
    "karena banyak sekali missing values pada data, maka akan dilakukan handling dengan menggunakan data_stage1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e1e0ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_stage1 = \u001b[43mdata\u001b[49m.copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data_stage1 = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687aa10",
   "metadata": {},
   "source": [
    "Selanjutnya adalah membagi data menjadi dua yaitu high priority (fitur yang logis banget untuk dipertahankan) dan low priority (fitur yang kurang berpengaruh), selanjutnya membagi berdasarkan low latency (kolom lengkap) dan high latency (banyak missing value) dengan variabel target yang digunakan adalah price_eur (memprediksi harga mobil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membagi menjadi dua priority yang mempengaruhi mahal dan murahnya mobil\n",
    "high_priority = data_stage1[['price_eur','mileage', 'engine_power','engine_displacement','fuel_type','transmission','stk_year']]\n",
    "low_priority = data_stage1[['model','body_type','door_count','seat_count','color_slug']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kembangkan def missing_values(data) yang kita miliki\n",
    "def missing_values_priority(data_stage1, cols):\n",
    "  missing_list = []\n",
    "  for col in cols:\n",
    "    missing_count = data_stage1[col].isnull().sum()\n",
    "    missing_percent = missing_count * 100/len(data_stage1)\n",
    "    missing_list.append([col, missing_count, missing_percent])\n",
    "    missing_table = pd.DataFrame(missing_list, columns=['Column', 'Missing Count', 'Missing Percentage'])\n",
    "    missing_table = missing_table.sort_values(by='Missing Percentage', ascending=False)\n",
    "  return missing_table\n",
    "\n",
    "missing_values_priority(data_stage1, list(high_priority.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc647a74",
   "metadata": {},
   "source": [
    "berdasarkan high_priority didapatkan bahwa yang termasuk high latency (missing values >= 50%) yaitu stk_year dan fuel_type, low_latency (missing values < 50%) yaitu engine displacement, transmission, engine_power, dan mileage. Dengan variabel target adalah price_eur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f48818",
   "metadata": {},
   "source": [
    "Dengan missing values yang memiliki low latency akan langsung melakukan impute missing values, seperti menggunakan impute median, mode, atau mean. Dengan aturan bahwa, impute menggunakan median apabila nilainya numeric normal dan tanpa outlier, impute dengan mean apabila nilainya numeric skewness atau terdapat outlier, dan smenggunakan mode apabila merupakan categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb094d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.dtypes.common import is_numeric_dtype\n",
    "def impute_strategy(data, outlier_threshold = 0.05, skewness_threshold = 0.5):\n",
    "  imputation_strategies = {}\n",
    "  data_stage1 = data.copy()\n",
    "  for col in data_stage1.columns:\n",
    "    missing_ratio = data_stage1[col].isnull().sum() / len(data_stage1)\n",
    "    if missing_ratio == 1.0:\n",
    "      print(f\"{col} has no missing values\")\n",
    "      continue\n",
    "    if pd.api.types.is_numeric_dtype(data_stage1[col]):\n",
    "      Q1 = data_stage1[col].quantile(0.25)\n",
    "      Q3 = data_stage1[col].quantile(0.75)\n",
    "      IQR = Q3 - Q1\n",
    "      outlier_mask = (data_stage1[col] < Q1 - 1.5 * IQR) | (data_stage1[col] > Q3 + 1.5 * IQR)\n",
    "      outlier_ratio = outlier_mask.mean()\n",
    "\n",
    "      if outlier_ratio > outlier_threshold:\n",
    "        strategy = 'median'\n",
    "        data_stage1[col] = data_stage1[col].fillna(data_stage1[col].median())\n",
    "        print(f\"{col} impute with {strategy}\")\n",
    "      else:\n",
    "        strategy = 'mean'\n",
    "        data_stage1[col] = data_stage1[col].fillna(data_stage1[col].mean())\n",
    "        print(f\"{col} impute with {strategy}\")\n",
    "    else:\n",
    "      strategy = 'mode'\n",
    "      data_stage1[col] = data_stage1[col].fillna(data_stage1[col].mode()[0])\n",
    "      print(f\"{col} impute with {strategy}\")\n",
    "  return data_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = list(high_priority.columns) + list(low_priority.columns)\n",
    "data_strategy = impute_strategy(data_stage1)\n",
    "data_strategy1 = data_strategy[selected_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the least\n",
    "data_stage1['mileage'] = data_stage1['mileage'].fillna(data_stage1['mileage'].median())\n",
    "data_stage1['engine_power'] = data_stage1['engine_power'].fillna(data_stage1['engine_power'].median())\n",
    "data_stage1['transmission'] = data_stage1['transmission'].fillna(data_stage1['transmission'].mode()[0])\n",
    "data_stage1['engine_displacement'] = data_stage1['engine_displacement'].fillna(data_stage1['engine_displacement'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{data_stage1['mileage'].isnull().sum()} missing values in mileage\")\n",
    "print(f\"{data_stage1['engine_power'].isnull().sum()} missing values in engine_power\")\n",
    "print(f\"{data_stage1['transmission'].isnull().sum()} missing values in transmission\")\n",
    "print(f\"{data_stage1['engine_displacement'].isnull().sum()} missing values in engine_displacement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f28cc",
   "metadata": {},
   "source": [
    "terdapat dua kolom dengan high latency yaitu stk_year dan fuel_type, dimana akan menggunakan rule based imputation.\n",
    "Dimana rule based imputation tidak berlaku apabila missing value lebih dari 50%, sebaiknya mengganti menjadi existed/not existed atau valid/not valid.\n",
    "\n",
    "Sehingga akan diubah stk_year ditransformasikan menjadi variabel biner yaitu existed vs not existed, serta fuel_type menjadi variabel biner yaitu known vs unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing value with high latency\n",
    "data_stage1['stk_year_existence']= np.where(data_stage1['stk_year'].isnull(), 'not existed', 'existed')\n",
    "data_stage1['fuel_type_knownness']= np.where(data_stage1['fuel_type'].isnull(), 'unknown', 'known')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbbf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_priority(data_stage1, list(low_priority.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3712767",
   "metadata": {},
   "source": [
    "Selanjutnya akan dilakukan drop kolom color_slug karena missing value yang sangat tinggi, yaitu hampir mencapai 100%. Drop dilakukan untuk menghindari risiko bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67affd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# akan dilakukan drop color_slug karena missing values \n",
    "low_priority = low_priority.drop(columns=['color_slug'])\n",
    "data_stage1 = data_stage1.drop(columns=['color_slug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the least \n",
    "data_stage1['door_count'] = data_stage1['door_count'].fillna(data_stage1['door_count'].median())\n",
    "data_stage1['body_type'] = data_stage1['body_type'].fillna(data_stage1['body_type'].mode()[0])\n",
    "data_stage1['model'] = data_stage1['model'].fillna(data_stage1['model'].mode()[0])\n",
    "data_stage1['seat_count'] = data_stage1['seat_count'].fillna(data_stage1['seat_count'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{data_stage1['door_count'].isnull().sum()} missing values in door_count\")\n",
    "print(f\"{data_stage1['body_type'].isnull().sum()} missing values in body_type\")\n",
    "print(f\"{data_stage1['model'].isnull().sum()} missing values in model\")\n",
    "print(f\"{data_stage1['seat_count'].isnull().sum()} missing values in seat_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_check (data_stage1):\n",
    "  missing_count = data_stage1.isnull().sum()\n",
    "  missing_percent = missing_count * 100 / len(data_stage1)\n",
    "  missing_table = pd.concat([missing_count, missing_percent], axis=1)\n",
    "  missing_table = missing_table.sort_values(by=1, ascending=False)\n",
    "  missing_table.columns = ['Missing Count', 'Missing Percentage']\n",
    "  return missing_table\n",
    "\n",
    "missing_check(data_stage1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099af198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selanjutnya akan drop stk_year dan fuel_type\n",
    "data_stage1 = data_stage1.drop(columns=['stk_year', 'fuel_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c71b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengubah tipe data tahun menjadi int64 \n",
    "data_stage1['manufacture_year'] = data_stage1['manufacture_year'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan imputasi missing value untuk manufacture_year dan maker\n",
    "data_stage1['manufacture_year'] = data_stage1['manufacture_year'].fillna(data_stage1['manufacture_year'].median())\n",
    "data_stage1['maker'] = data_stage1['maker'].fillna(data_stage1['maker'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_check(data_stage1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5d543",
   "metadata": {},
   "source": [
    "Setelah data dibersihkan, baseline model akan dibangun sebagai pembanding awal.\n",
    "One-hot encoding diterapkan pada beberapa variabel kategorikal agar dapat digunakan dalam baseline model, yaitu model regresi.\n",
    "\n",
    "Sebelumnya, data_stage1 akan dibagi menjadi 2 yaitu cat_cols dan num_cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea86d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = data_stage1.select_dtypes(include=['object', 'category'])\n",
    "num_cols = data_stage1.select_dtypes(exclude = ['object','category'])\n",
    "print(cat_cols.columns)\n",
    "print(num_cols.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f87b7",
   "metadata": {},
   "source": [
    "Karena kursi dan pintu mobil tidak mungkin decimal, maka seat_count dan door_count akan diubah tipe data menjadi Int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96683d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1['seat_count'] = data_stage1['seat_count'].astype('Int64')\n",
    "data_stage1['door_count'] = data_stage1['door_count'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a8465",
   "metadata": {},
   "source": [
    "Terdapat ketidaksesuaian tipe data pada kolom date_created dan date_last_seen. Kolom date_created mencatat waktu pengambilan data (scraping), sedangkan date_last_seen mencatat waktu terakhir iklan terlihat (sesuai kebijakan, seluruh data iklan yang berusia lebih dari 60 hari akan dihapus dari dataset).\n",
    "\n",
    "Sehingga perlu dilakukan drop date_created dan date_last_seen dari cat_cols \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = cat_cols.drop(columns=['date_created','date_last_seen'])\n",
    "cat_cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005565ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_summary (data_stage1, cols):\n",
    "  for col in cols:\n",
    "    print(f\"Column : {col}\")\n",
    "    print(f\"Unique values : {data_stage1[col].nunique()}\")\n",
    "    print(f\"Missing values : {data_stage1[col].isnull().sum()}\")\n",
    "    print(data_stage1[col].value_counts(normalize = True, dropna = False))\n",
    "    print(\"-\"*40)\n",
    "cat_summary(data_stage1, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee16aaf",
   "metadata": {},
   "source": [
    "Karena transmission, stk_year_existence, dan fuel_type_knowness sudah menggunakan variabel biner maka langsung dapat mengubah tipe datanya menjadi category dan dilakukan ohe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1['transmission'] = data_stage1['transmission'].astype('category')\n",
    "data_stage1['stk_year_existence'] = data_stage1['stk_year_existence'].astype('category')\n",
    "data_stage1['fuel_type_knownness'] = data_stage1['fuel_type_knownness'].astype('category')\n",
    "\n",
    "data_stage1 = pd.get_dummies(data_stage1, columns = ['transmission','stk_year_existence', 'fuel_type_knownness'], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda22a57",
   "metadata": {},
   "source": [
    "Karena kolom maker dan model memiliki terlalu banyak nilai unik (unique values), konversi langsung tipenya menjadi category tidak disarankan. Pendekatan yang digunakan adalah membatasi data menjadi top 10 kategori utama, sementara sisanya dikelompokkan ke dalam label 'Other'. Sedangkan, kolom body_type dapat langsung diubah tipenya menjadi category dan diterapkan teknik One-Hot Encoding (OHE) karena jumlah kategorinya yang rendah (<=10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1['body_type'] = data_stage1['body_type'].astype('category')\n",
    "data_stage1 = pd.get_dummies(data_stage1, columns = ['body_type'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00550cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_categories(data_stage1, col, n=9, other_label='Other'):\n",
    "  top_categories = data_stage1[col].value_counts().nlargest(n).index\n",
    "  data_stage1[col] = data_stage1[col].where(data_stage1[col].isin(top_categories), other_label)\n",
    "  return data_stage1\n",
    "\n",
    "top_n_categories(data_stage1, 'maker', n=9, other_label='Other')\n",
    "top_n_categories(data_stage1, 'model', n=9, other_label='Other')\n",
    "\n",
    "data_stage1 = pd.get_dummies(data_stage1, columns = ['maker','model'], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08e5a1",
   "metadata": {},
   "source": [
    "Selanjutnya akan melakukan split data untuk pembuatan dan pengujian model, model pertama yang dibentuk adalah baseline model, yaitu linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a714e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untuk linear regression akan menggunakan OHE\n",
    "cat_cols = data_stage1.select_dtypes(include=['category','object']).columns.tolist()\n",
    "cat_cols = [col for col in cat_cols if col not in ['date_created','date_last_seen']] # tanggal tidak masuk\n",
    "\n",
    "num_cols = data_stage1.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col not in ['price_eur','date_created','date_last_seen']]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output = False, drop = 'first', handle_unknown = 'ignore')\n",
    "X_cat_ohe = ohe.fit_transform(data_stage1[cat_cols])\n",
    "X_cat_ohe = pd.DataFrame(X_cat_ohe, columns = ohe.get_feature_names_out(cat_cols), index = data_stage1.index)\n",
    "\n",
    "X_num_ohe = data_stage1[num_cols]\n",
    "X = pd.concat([X_cat_ohe, X_num_ohe], axis = 1)\n",
    "y = data_stage1['price_eur']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split \n",
    "\n",
    "lr = LinearRegression() # fit linear regression\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hitung  metric pada satu split test set\n",
    "# mengecek performa actual prediction\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"=== Test Set Metrics ===\")\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"R2   : {r2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7048219",
   "metadata": {},
   "source": [
    "Didapatkan bahwa nilai R^2 dibawah 0, berarti model baseline yang dibuat belum sesuai atau berguna, maka perlu dilakukan pembuatan ulang model baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1['price_eur'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a9600",
   "metadata": {},
   "source": [
    "Ternyata terdapat permasalahan pada kolom price_eur, didapatkan bahwa harga mobil mengalami right-skewed (nilai >1). Kemudian, akan dilakukan log-transform pada variabel target(y) dan  melihat apakah ada peningkatan pada nilai r2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc234fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=14khOIntWRDhbrE82xsz1iLTLn6P_DoQx\n",
      "From (redirected): https://drive.google.com/uc?id=14khOIntWRDhbrE82xsz1iLTLn6P_DoQx&confirm=t&uuid=93986da9-d810-4fbf-bd1e-5fdc05160adf\n",
      "To: d:\\portfolio-projects-learning\\data.csv\n",
      "100%|██████████| 419M/419M [00:22<00:00, 19.0MB/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
      "\u001b[32m      3\u001b[39m output = \u001b[33m'\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m      4\u001b[39m gdown.download(url, output, quiet=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m      6\u001b[39m data.head()\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# pembentukan ulang model baseline (dengan transform log pada variabel target)\n",
    "X_log = pd.concat([X_cat_ohe, X_num_ohe], axis = 1)\n",
    "y_log = np.log1p(data_stage1['price_eur'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_log, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_log = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ecb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balik log untuk metric interpretasi\n",
    "y_test_exp = np.expm1(y_test)\n",
    "y_pred_exp = np.expm1(y_pred_log)\n",
    "\n",
    "# hitung metric\n",
    "mae = mean_absolute_error(y_test_exp, y_pred_exp)\n",
    "mse = mean_squared_error(y_test_exp, y_pred_exp)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_exp, y_pred_exp)\n",
    "\n",
    "print(\"Test Set Metrics\")\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"R2   : {r2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cf54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_mean = data_stage1['price_eur'].mean()\n",
    "mae_percent = mae / price_mean * 100\n",
    "rmse_percent = rmse / price_mean * 100\n",
    "\n",
    "print(f\"MAE : {mae_percent:.2f}% of mean price\")\n",
    "print(f\"RMSE: {rmse_percent:.2f}% of mean price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd37585",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1['price_eur'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e7ce0",
   "metadata": {},
   "source": [
    "Dapat terlihat bahwa setelah melakukan log pada variabel target nilai R2 tidak memiliki peningkatan performa. Dimana dapat terlihat bahwa nilai MAE sudah cukup sesuai (prediksi rata rata melesek 25% dari harga rata rata), cukup menjadi toleransi baseline model.\n",
    "\n",
    "Terdapat permasalahan yang signifikan pada RMSE yang nilainya sangat tinggi, hal ini terjadi karena adanya outlier super ekstrem yang membuat SE tinggi. Kemudian variabel target yang kita miliki lebih dari batas normal, mengindikasikan terdapat right-skewness pada variabel target kita.\n",
    "\n",
    "Cara improvement dapat dilakukan dengan melakukan penangan pada outlier, transformasi target, seperti Box-cos atau Yeo-Johnson, serta coba  melakukan feature engineering jika diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling outlier pada data_stage1['price_eur']\n",
    "# check distribution using histogram\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.histplot(data_stage1['price_eur'], bins=50, kde=True)\n",
    "plt.title('Distribution of price_eur')\n",
    "plt.xlabel('Price EUR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0012f",
   "metadata": {},
   "source": [
    "Dapat dilihat dari hasil plot, bahwa price_eur sangat skewed ke kanan (right-skewed) dan ada outlier yang sangat ekstrem, yaitu memiliki nilai hingga trilliunan. Sehingga hal itu yang membuat log transform tidak bberhasil untuk meningkatkan performa baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dacdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1['price_eur'].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5208e1",
   "metadata": {},
   "source": [
    "Nilai sudah dipastikan bukan merupakan nilai yang salah tulis, hal ini mungkin terjadi karena ada pembelian mobil dengan harga yang sangat mahal. \n",
    "\n",
    "Cara penangannya adalah kita dapat melakukan capping/winsorization (misal 1%-99% percentile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b28b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan capping/winsorization\n",
    "lower = data_stage1['price_eur'].quantile(0.01)\n",
    "upper = data_stage1['price_eur'].quantile(0.99)\n",
    "\n",
    "data_stage1['price_eur_capped'] = data_stage1['price_eur'].clip(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck distribution after capping\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data_stage1['price_eur_capped'],bins=50, kde=True)\n",
    "plt.title('Distribution of price_eur_capped')\n",
    "plt.xlabel('Price EUR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe55aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage1['price_eur_capped'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a5a1c",
   "metadata": {},
   "source": [
    "Dapat dilihat bahwa skewness mengalami penurunan nilai, dari nilai awal 1323.25 menjadi 2.02, dimana teknik capping sudah menjadi langkah yang tepat.\n",
    "\n",
    "Langkah selanjutnya karena masih berada skewness > 1, maka akan dilakukan pertimbangan box-cox (skewness < 0.5 sudah cukup normal, tidak perlu melakukan transformasi Box-Cox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c713ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan Box-Cox transform\n",
    "price = data_stage1['price_eur_capped'] + 1\n",
    "price_boxcox, fitted_lambda = stats.boxcox(price)\n",
    "print(f\"Fitted lambda value: {fitted_lambda}\")\n",
    "\n",
    "sns.histplot(price_boxcox, bins=50,kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3354a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(price_boxcox).skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a57386",
   "metadata": {},
   "source": [
    "Didapatkan setelah melakukan transformasi dengan box_cox, dimana price_eur sudah cukup simetris, dengan nilai -0.04774 sudah mendekati nilai 0.\n",
    "\n",
    "Langkah selanjutnya, dilakukan pembuatan ulang baseline model yaitu linear regression menggunakan price baru yaitu price_boxcox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pembuatan ulang baseline model\n",
    "cat_cols = data_stage1.select_dtypes(include=['category','object']).columns.tolist()\n",
    "cat_cols = [col for col in cat_cols if col not in ['date_created','date_last_seen']] # tanggal tidak masuk\n",
    "\n",
    "num_cols = data_stage1.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "num_cols = [col for col in num_cols if col not in ['price_eur','date_created','date_last_seen']]\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output = False, drop = 'first', handle_unknown = 'ignore')\n",
    "X_cat_ohe = ohe.fit_transform(data_stage1[cat_cols])\n",
    "X_cat_ohe = pd.DataFrame(X_cat_ohe, columns = ohe.get_feature_names_out(cat_cols), index = data_stage1.index)\n",
    "\n",
    "X_num_ohe = data_stage1[num_cols]\n",
    "X = pd.concat([X_cat_ohe, X_num_ohe], axis = 1)\n",
    "y = price_boxcox\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c12428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hitung metric model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Set Metrics\")\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"R2   : {r2:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25a99",
   "metadata": {},
   "source": [
    "Didapatkan nilai R2 dari model dengan variabel target menggunakan price_boxcox sudah sesuai, yaitu bernilai 0.77. MAE dan RMSE dari model baru sudah dibawah 2, sehingga dapat dikatakan bahwa skewness sudah ditangani cukup baik.\n",
    "\n",
    "Selanjutnya, kita akan melakukan pengecekan reabilitas baseline model yang kita miliki dengan menggunakan CV (crossfold-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek reabilitas model dengan cv_scores\n",
    "X = pd.concat([X_cat_ohe, X_num_ohe], axis = 1)\n",
    "y = price_boxcox\n",
    "model = LinearRegression()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f\"R2 scores per fold: {cv_scores}\")\n",
    "print(f\"Mean CV R2: {np.mean(cv_scores):.5f}\")\n",
    "print(f\"Std CV R2: {np.std(cv_scores):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d043a",
   "metadata": {},
   "source": [
    "Didapatkan bahwa mean r2 memiliki nilai mendekati 1, dimana dapat dikatakan bahwa model fit cukup bagus. Dengan std R2, bernilai kecil dimana menandakan bahwa model konsisten, tidak mengalami overfitting.\n",
    "\n",
    "Mean r2 digunakan untuk melihat seberapa bagus model secara rata rata. Dengan Std r2 berguna untuk melihat sbeerapa stabil performa di fold berbeda. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de70cab",
   "metadata": {},
   "source": [
    "Walaupun nilai r2 sudah cukup baik, kita dapat melakukan peningkatan dengan membuat melakukan pembuatan model lain yaitu RandomForest dengan tuning, XGBoost dengan tuning. Selain itu, pertimbangkan feature engineering jika diperlukan.\n",
    "\n",
    "Dilakukan Ridge/Lasso atau regularisasi apabila datanya overfit, dimana melakukan penambahan penalty supaya model gak overfit, terutama jika terjadi banyak fitu/multikolinearitas, dengan dataset yang lebih kecil dibandingkan jumlah fitur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest dengan tuning hyperparameter menggunakan GridSearchCV\n",
    "X = pd.concat([X_cat_ohe, X_num_ohe], axis = 1)\n",
    "y = price_boxcox\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# tentukan grid parameter untuk tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "# GridSearchCV untuk 5-fold CV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='r2', n_jobs = -1, verbose = 1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best R2 score: {grid_search.best_score_:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hitung metric model rf after tuning\n",
    "y_pred = grid_search.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Set Metrics (RF after Tuning)\")\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"R2   : {r2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model XGBoost dengan tuning hyperparameter menggunakan GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3,5,7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='r2', n_jobs = -1, verbose = 1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best R2 score: {grid_search.best_score_:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung metric model xgboost after tuning\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Set Metrics (XGBoost after Tuning)\")\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"MSE  : {mse:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"R2   : {r2:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
